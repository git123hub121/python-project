{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:16.982660Z",
     "start_time": "2020-11-15T12:29:16.905124Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv does not exist: '/Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-10dfb1a12883>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comment_voted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comment_voted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'movie_star'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comment_time'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv does not exist: '/Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/wangjia/Documents/2.技术公号/公号项目/爬虫/豆瓣/35163988.csv\")\n",
    "df = df[['user_name','comment_voted','comment_voted','movie_star','comment_time','comment']]\n",
    "df.head(10)"
   ]
  },
  {
   "source": [
    "## 查看数据信息"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:17.004263Z",
     "start_time": "2020-11-15T12:29:16.986229Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字段类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:17.057037Z",
     "start_time": "2020-11-15T12:29:17.012911Z"
    }
   },
   "outputs": [],
   "source": [
    "df['comment_time'] = pd.to_datetime(df['comment_time'])\n",
    "df[\"comment\"] = df[\"comment\"].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机械压缩去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:17.101997Z",
     "start_time": "2020-11-15T12:29:17.070120Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义机械压缩函数\n",
    "def yasuo(st):\n",
    "    for i in range(1,int(len(st)/2)+1):\n",
    "        for j in range(len(st)):\n",
    "            if st[j:j+i] == st[j+i:j+2*i]:\n",
    "                k = j + i\n",
    "                while st[k:k+i] == st[k+i:k+2*i] and k<len(st):   \n",
    "                    k = k + i\n",
    "                st = st[:j] + st[k:]    \n",
    "    return st\n",
    "yasuo(st=\"菜J学Python真的真的真的很菜很菜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:18.814441Z",
     "start_time": "2020-11-15T12:29:17.107456Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"comment\"] = df[\"comment\"].apply(yasuo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T10:21:48.832961Z",
     "start_time": "2020-11-15T10:21:48.811061Z"
    }
   },
   "source": [
    "## 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:18.826107Z",
     "start_time": "2020-11-15T12:29:18.816958Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "不久之前，百度正式发布情感预训练模型 SKEP (Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis)。\n",
    "通过利用情感知识增强预训练模型，SKEP 在 14 项中英情感分析典型任务上全面超越 SOTA。\n",
    "\n",
    "具体实现原理，详见「SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis」。\n",
    "paddlehub,预训练模型管理和迁移学习组件，10行代码完成迁移学习。提供40+预训练模型，\n",
    "覆盖文本、图像、视频三大领域八类模型；模型即软件，通过Python API或者命令行工具，\n",
    "一行代码完成预训练模型的预测；结合Fine-tune API，10行代码完成迁移学习。 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:29:35.291125Z",
     "start_time": "2020-11-15T12:29:18.832349Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip3 install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n",
    "import paddlehub as hub\n",
    "##这里使用了百度开源的成熟NLP模型来预测情感倾向\n",
    "senta = hub.Module(name=\"senta_bilstm\")\n",
    "texts = df['comment'].tolist()\n",
    "input_data = {'text':texts}\n",
    "res = senta.sentiment_classify(data=input_data)\n",
    "df['pos_p'] = [x['positive_probs'] for x in res]\n",
    "##重采样至五分钟\n",
    "# df.index = df['comment_time']\n",
    "# data = df.resample('15min').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T12:37:21.111888Z",
     "start_time": "2020-11-15T12:37:21.082335Z"
    }
   },
   "outputs": [],
   "source": [
    "df[[\"comment\",\"pos_p\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T14:40:52.323496Z",
     "start_time": "2020-11-15T14:40:52.291485Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T14:46:03.418853Z",
     "start_time": "2020-11-15T14:46:03.402352Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df['month'] = df[\"comment_time\"].dt.month\n",
    "df['hour'] = df[\"comment_time\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T14:46:04.041348Z",
     "start_time": "2020-11-15T14:46:03.976190Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:03:18.589896Z",
     "start_time": "2020-11-15T15:03:16.380423Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.to_excel(\"/Users/wangjia/Desktop/clean.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:36:36.560904Z",
     "start_time": "2020-11-15T15:36:31.242488Z"
    }
   },
   "outputs": [],
   "source": [
    "#数据处理库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import jieba \n",
    "\n",
    "#可视化库\n",
    "import stylecloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from pyecharts.charts import *\n",
    "from pyecharts import options as opts \n",
    "from pyecharts.globals import ThemeType  \n",
    "from IPython.display import Image \n",
    "df['comment'] = df['comment'].astype('str')\n",
    "# 定义分词函数\n",
    "def get_cut_words(content_series):\n",
    "    # 读入停用词表\n",
    "    stop_words = [] \n",
    "    \n",
    "    with open(\"/Users/wangjia/Documents/2.技术公号/公号项目/数据分析/豆果美食数据分析/stop_words.txt\", 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            stop_words.append(line.strip())\n",
    "\n",
    "    # 添加关键词\n",
    "    my_words = ['', '']  \n",
    "    \n",
    "    for i in my_words:\n",
    "        jieba.add_word(i) \n",
    "\n",
    "    # 自定义停用词\n",
    "    my_stop_words = ['节目', '中国','一部']   \n",
    "    stop_words.extend(my_stop_words)               \n",
    "\n",
    "    # 分词\n",
    "    word_num = jieba.lcut(content_series.str.cat(sep='。'), cut_all=False)\n",
    "\n",
    "    # 条件筛选\n",
    "    word_num_selected = [i for i in word_num if i not in stop_words and len(i)>=2]\n",
    "    \n",
    "    return word_num_selected\n",
    "\n",
    "# 绘制词云图\n",
    "text1 = get_cut_words(content_series=df['comment'])\n",
    "stylecloud.gen_stylecloud(text=' '.join(text1), max_words=200,\n",
    "                          collocations=False,\n",
    "                          font_path='字酷堂清楷体.ttf',\n",
    "                          icon_name='fas fa-video',\n",
    "                          size=653,\n",
    "                          #palette='matplotlib.Inferno_9',\n",
    "                          output_name='./1.png')\n",
    "Image(filename='./1.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:39:39.352119Z",
     "start_time": "2020-11-15T15:39:39.330222Z"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in df['comment']:\n",
    "    result.append(re.split('[:：,，.。!！~·`\\;； ……、]',i))\n",
    "    \n",
    "def actor_comment(df,result):\n",
    "    actors = pd.DataFrame(np.zeros(5 * len(df)).reshape(len(df),5),\n",
    "                      columns = ['郭敬明','赵薇','陈凯歌','尔冬升','李诚儒'])\n",
    "    for i in range(len(result)):\n",
    "        words = result[i]\n",
    "        for word in words:\n",
    "            if '郭敬明' in word or '郭敬明' in word: \n",
    "                actors.iloc[i]['郭敬明'] = 1\n",
    "            if '赵薇' in word or '赵薇' in word:              \n",
    "                actors.iloc[i]['赵薇'] = 1 \n",
    "            if '陈凯歌' in word or '陈凯歌' in word:              \n",
    "                actors.iloc[i]['陈凯歌'] = 1 \n",
    "            if '尔冬升' in word or '尔冬升' in word: \n",
    "                actors.iloc[i]['尔冬升'] = 1\n",
    "            if '李诚儒' in word or '李诚儒' in word:              \n",
    "                actors.iloc[i]['李诚儒'] = 1 \n",
    "    final_result = pd.concat([df,actors],axis = 1)\n",
    "    return final_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:42:39.401675Z",
     "start_time": "2020-11-15T15:42:39.176713Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = actor_comment(df,result)\n",
    "df3.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:43:09.816303Z",
     "start_time": "2020-11-15T15:43:09.795289Z"
    }
   },
   "outputs": [],
   "source": [
    "df4 = df3.iloc[:,10:].sum().reset_index().sort_values(0,ascending = False)\n",
    "df4.columns = ['角色','次数']\n",
    "df4['占比'] = df4['次数'] / df4['次数'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T15:43:13.861164Z",
     "start_time": "2020-11-15T15:43:13.842208Z"
    }
   },
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}